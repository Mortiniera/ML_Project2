
	1. TEXT PREPROCESSING




1.1 Tokenization 

split input sequences into tokens. Here we will split each twits into sequence of words. (whitespace tokenizer in python library NLTK). 
	ex : nltk.tokenize.TreebankWordTokenizer() or WordPunctTokenizer(). 
Test both and see which one gives better results.

Demo :
https://github.com/hse-aml/natural-language-processing/blob/master/week1/lemmatization_demo.ipynb



1.2 Token normalization

we may want same token for different forms of the word. (ex : wolf, wolves -> wolf)
In order to do so, we will use stemming or Lemmatization

	Stemming : Removing and replacing suffixes to get root form of the word.
	ntlk.ste.PorterStemmer
	Issues : fails on the regular forms and produces non-words. 
	Ex : ponies -> poni instead of pony and feet -> feet instead of foot

	Lemmatizer : Ues WordNet Database to lookup lemmas
	ntlk.ste.WordNetLemmatizer
	Better, however not all forms are reduced. talked -> talked. 

Try both and choose which one works best for our task.



1.3 Further normalization

some problems might occur. Example how to distinguish between us(pronoun) and US (country).

How to tackle it ? 

Use of heuristics :
	- lowercase beginning of sentence bc we know that each sentence starts with capital letter
	- lowercase words in title
	- leave mid-sentence words as they are (maybe they are names of something)

Use acronyms with regular expression (hard to do in practive. Need to know all of them in advance)
	- for ex : eta, e.t.a, E.T.A -> E.T.A


	


	2 . FEATURE EXTRACTION FROM TEXT (transforms tokens into features)


Bag of Words

count occurences of tokens in our text. We are looking for marker words like "excellent" of "disappointed" for example.
For each token, we will have a feature column : text vectorization. (1 in column i if feature i appear in the textm 0 otherwise)

However, we loose order :
	-> Use columns that corresponds to token pairs instead (not moovie and good, but also good 	moovie). Helps to preserve local word order. 
Problem : too many features !!
To overcome that, lets remove some n-grams features based on their occurence freq in the whole document.

We don't need to high freq n-grams and low-freq n-grams (stop-words and typos. don't need them or we will overfit)
Hence keep medium frequency n-grams. which one are good ?
The n-gram with smaller freq can be more discriminating bc it can capture a specific issue in the review. 

In order to capture this idea, we will use TF-IDF
 
TERM FREQUENCY (tf)
tf(t, d) : token freq in doc D
Use either term freq : f(t, d) / sum(f(t', d)) OR log normalization 1 + log(f(t, d)) with f(t,d) being the raw count of t in d.


INVERSE DOCUMENT FREQUENCY (IDF)
idf(t, D) = log(N / number of docs (tweets for us) where t appears) with N = number of docs in corpus (number of tweets ) 

TF-IDF VALUE
tfidf(t, d, D) = tf(t, d) * idf(t, D)
high weight is reached by high term freq and low doc freq of the term in whole corpus.
IDEA : find frequent terms in tweets that are not so frequent in whole data set.
So replace counters with TF-IDF and normalize results row-wise (divide by L2 norm)

https://github.com/hse-aml/natural-language-processing/blob/master/week1/tfidf_demo.ipynb



	3. LINEAR MODELS FOR SENTIMENT ANALYSIS
As a measure, we can use accuracy if our whole data set is balanced (unbiased) : same number of neg or positive emojis. Random classifier : 50% of accuracy


we expect to have huge BUT sparse matrix.

Use logistic regression.
Linear classification model
Since its linear it can handle sparse data and weights can be interpreted. (positive weights mapped to positive emojis, neg ones to negative emojis.) (PRINT TOP POS WEIGHTS WITH CORRESPONDING N GRAMS AND TOP NEG)
Try 1-gram first then two grams. (throw away words seen less than 5 times for ex)
Should see bump of accuracy between 1 and 2 grams.

IDEAS TO IMPROVE ACCURACY :
- play with tokenization
- normalize tokensLinear
- try different models : SVM, naive bayes,...
- throw everything and use deep learning







