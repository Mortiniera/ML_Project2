{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter \n",
    "from matplotlib.colors import ListedColormap\n",
    "import string\n",
    "import re\n",
    "from scipy.stats import hmean\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline\n",
    "pth = r\"C:\\EPFL\\2018-2019\\nltk_data\" #change location according to your nltk data path\n",
    "nltk.data.path.append(pth)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#from sklearn.model_selection.cross_validation import train_test_split\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of a pipeline. Used to apply sequence of transform. I had issue with cvec, needed to apply it two times on x_train and x_test, it gave bad results...**\n",
    "\n",
    "https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n",
    "https://stackoverflow.com/questions/43366561/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once\n",
    "\n",
    "https://www.kaggle.com/cesartrevisan/scikit-learn-and-gridsearchcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Classifier Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best results we got so far were with tf-idf (even if the difference was not that significant with Countvectorizer) along with 100 000 features and use of unigram and bigram. Now we will compare using the same pipeline other models. Due to the expensive computation time, we will compare these models with a maximum of 10 000 features only and using search grid to get the best estimator each time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import multiprocessing\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC #support vector machine SVM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('clean_tweets')\n",
    "X = data.text.values\n",
    "y = data.sentiment.values\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Linear SVC\", \"Random Forest\",\"Ridge Classifier\"]\n",
    "\n",
    "classifiers = [LogisticRegression(),LinearSVC(), RandomForestClassifier(), RidgeClassifier()]\n",
    "zipped_clf = zip(names,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext cythonmagic\n",
    "def classifier_comparator(vectorizer, ngram_range, classifier_list, n_features=10000, stop_words=None):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier_list:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        \n",
    "        #----------- SEARCH GRID -----------\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Choosen classifier : {}\".format(n))\n",
    "        t0 = time()\n",
    "        if(n == \"Logistic Regression\") :\n",
    "            hyperparameters = dict(C=np.logspace(0, 4, 10), penalty=['l1', 'l2'])\n",
    "            clf = GridSearchCV(c, hyperparameters, cv=5, verbose=0)\n",
    "        elif(n == \"Random Forest\") :\n",
    "            hyperparameters = {\"max_depth\": [3, None]}\n",
    "        elif(n == \"Ridge Classifier\") :\n",
    "            hyperparameters = dict(alpha = np.array([0.01,0.001,0.0001]))\n",
    "        elif(n == \"Linear SVC\") :\n",
    "            Cs = [0.01, 0.1, 1]\n",
    "            hyperparameters = {'C': Cs}\n",
    "        \n",
    "        #search for best estimator\n",
    "        clf = GridSearchCV(c, hyperparameters, cv=5, verbose=0)\n",
    "        vectorizer.set_params(max_features=n_features, ngram_range=ngram_range)\n",
    "        best_estimator = Pipeline([('vectorizer', vectorizer),('classifier', clf)])\n",
    "        #best_estimator = Pipeline([('vectorizer', vectorizer),('classifier', c)])\n",
    "\n",
    "        #---------- predict with best model ------------\n",
    "        classifier_fit = best_estimator.fit(X_train, y_train)\n",
    "        y_pred = classifier_fit.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        tt_time = time() - t0\n",
    "        result.append([n, accuracy, tt_time])\n",
    "        print(\"Accuracy score is {} :\".format(accuracy))\n",
    "        print(\"Train and test time took {}\".format(tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Logistic Regression\n",
      "Accuracy score is 0.817795856107792 :\n",
      "Train and test time took 605.3928697109222\n",
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Linear SVC\n",
      "Accuracy score is 0.8174145163340536 :\n",
      "Train and test time took 40.70728349685669\n",
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Random Forest\n",
      "Accuracy score is 0.7755688318291598 :\n",
      "Train and test time took 321.2097415924072\n",
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Ridge Classifier\n",
      "Accuracy score is 0.8126859031396975 :\n",
      "Train and test time took 461.7082438468933\n",
      "Wall time: 23min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pool = multiprocessing.Pool(processes=2)\n",
    "#r = pool.map(classifier_comparator, years)\n",
    "#pool.close()\n",
    "all_results = classifier_comparator(tvec, range(1, 3), zipped_clf, n_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289055548493708\n",
      "Wall time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
    "hyperparameters = {'C': Cs}\n",
    "c = GridSearchCV(LinearSVC(), hyperparameters, cv=5, verbose=0)\n",
    "vec = TfidfVectorizer()\n",
    "vec.set_params(max_features=100000, ngram_range=(1, 3))\n",
    "best_estimator = Pipeline([('vectorizer', vec),('classifier', c)])\n",
    "classifier_fit = best_estimator.fit(X_train, y_train)\n",
    "y_pred = classifier_fit.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stats.stackexchange.com/questions/17711/why-does-ridge-regression-classifier-work-quite-well-for-text-classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8289564001525359\n",
      "Wall time: 5min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#------------ SVC ------------------------------\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "hyperparameters = {'C': Cs}\n",
    "c = GridSearchCV(LinearSVC(), hyperparameters, cv=5, verbose=0)\n",
    "vec = TfidfVectorizer()\n",
    "vec.set_params(max_features=100000, ngram_range=(1, 1))\n",
    "best_estimator = Pipeline([('vectorizer', vec),('classifier', c)])\n",
    "\n",
    "#------------- SVC2 --------------\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "hyperparameters = {'C': Cs}\n",
    "c = GridSearchCV(LinearSVC(), hyperparameters, cv=5, verbose=0)\n",
    "vec = TfidfVectorizer()\n",
    "vec.set_params(max_features=100000, ngram_range=(1, 3))\n",
    "best_estimator2 = Pipeline([('vectorizer', vec),('classifier', c)])\n",
    "\n",
    "#------------- SCV3 ----------------\n",
    "Cs = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "hyperparameters = {'C': Cs}\n",
    "c = GridSearchCV(LinearSVC(), hyperparameters, cv=5, verbose=0)\n",
    "vec = TfidfVectorizer()\n",
    "vec.set_params(max_features=100000, ngram_range=(1, 4))\n",
    "best_estimator3 = Pipeline([('vectorizer', vec),('classifier', c)])\n",
    "\n",
    "#----------- Voting classifier ------------\n",
    "eclf = VotingClassifier(estimators=[('svc1', best_estimator), ('svc2', best_estimator2), ('svc3', best_estimator3)], voting='hard')\n",
    "\n",
    "\n",
    "classifier_fit = eclf.fit(X_train, y_train)\n",
    "y_pred = classifier_fit.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to test on Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load train and test data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'twitter-datasets'\n",
    "train_pos = pd.read_table(data_folder + '/train_pos_full.txt', header = None, names=['text'], sep='\\n')\n",
    "train_pos['sentiment'] = 'positive'\n",
    "train_neg = pd.read_table(data_folder + '/train_neg_full.txt', header = None, names=['text'], sep='\\n')\n",
    "train_neg['sentiment'] = 'negative'\n",
    "train_data = pd.concat([train_neg, train_pos]) #merge positive and negative tweets\n",
    "train_data = train_data.sample(frac=1).reset_index(drop=True) #shuffle the datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_folder + '/test_data.txt') as f:\n",
    "    lines =  [(line.rstrip('\\n').split(',', 1)) for line in f]\n",
    "test_data = (pd.DataFrame(lines, columns=['id', 'text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data cleaning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = [r'<user>', r'<url>', r'#', r'[0-9]', r'\\.', r'\\,', r'\\-', r'\\(', r'\\)', r'\\/'\\\n",
    "       , r'rt', r' \\: ', r' \\~ ', r' \\* ', r'<', r'>', r' \\\" ', r' \\' ']\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "def tweet_cleaner(text) : \n",
    "    allpat = text\n",
    "    for p1 in pat :\n",
    "        allpat = re.sub(p1,'',allpat)    \n",
    "    \n",
    "    allpat =  allpat.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], allpat)\n",
    "    words = neg_handled.split()\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data = pd.DataFrame(train_data.apply(lambda row : pd.Series([tweet_cleaner(row[0]), row[1]]), axis=1)) #clean each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kind of sad to think that .. after this final ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;user&gt; thank you ! x luv lots</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;user&gt; you can have mine lol</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rt &lt;user&gt; i just want you to know , that \" i l...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pong 360 official portable beer pong table - 8...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text sentiment\n",
       "0  kind of sad to think that .. after this final ...  negative\n",
       "1                      <user> thank you ! x luv lots  positive\n",
       "2                       <user> you can have mine lol  positive\n",
       "3  rt <user> i just want you to know , that \" i l...  positive\n",
       "4  pong 360 official portable beer pong table - 8...  negative"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet_texts = []\n",
    "for i in range(len(train_data)) :\n",
    "    clean_tweet_texts.append(tweet_cleaner(train_data['text'][i]))\n",
    "clean_df = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_df['sentiment'] = train_data.sentiment\n",
    "train_data = clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweet_texts = []\n",
    "for i in range(len(test_data)) :\n",
    "    clean_tweet_texts.append(tweet_cleaner(test_data['text'][i]))\n",
    "clean_ = pd.DataFrame(clean_tweet_texts,columns=['text'])\n",
    "clean_['id'] = test_data.id\n",
    "test_data = clean_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train our model : We will use SVC along with tdf-if and 100k features without removing stopwords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mapping -1 to negative sentiment and +1 to positive sentiment\n",
    "train_data.columns=['text', 'sentiment']\n",
    "test_data.columns=['text', 'id']\n",
    "train_data['sentiment'] = np.where(train_data['sentiment'] == 'negative', -1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data.text.values\n",
    "y_train = train_data.sentiment.values\n",
    "X_test = test_data.text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cs = [0.0001, 0.001, 0.01, 0.1, 1]\n",
    "#hyperparameters = {'C': Cs}\n",
    "hyperparameters = dict(C=np.logspace(0, 4, 10), penalty=['l1', 'l2'])\n",
    "#c = GridSearchCV(LinearSVC(), hyperparameters, cv=5, verbose=0)\n",
    "#c = RandomizedSearchCV(LinearSVC(), hyperparameters, cv=3, verbose=0, n_jobs=2)\n",
    "c = RandomizedSearchCV(LogisticRegression(), hyperparameters, cv=3, verbose=0, n_jobs=2)\n",
    "vec = TfidfVectorizer()\n",
    "vec.set_params(max_features=100000, ngram_range=(1, 3))\n",
    "best_estimator = Pipeline([('vectorizer', vec),('classifier', c)])\n",
    "classifier_fit = best_estimator.fit(X_train, y_train)\n",
    "y_pred = classifier_fit.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = test_data.id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('submit2.csv', 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Id', 'Prediction']\n",
    "        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for r1, r2 in zip(ids, y_pred):\n",
    "            writer.writerow({'Id':int(r1),'Prediction':int(r2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
