{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter \n",
    "from matplotlib.colors import ListedColormap\n",
    "import string\n",
    "import re\n",
    "from scipy.stats import hmean\n",
    "from scipy.stats import norm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "%matplotlib inline\n",
    "pth = r\"C:\\EPFL\\2018-2019\\nltk_data\" #change location according to your nltk data path\n",
    "nltk.data.path.append(pth)\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "#from sklearn.model_selection.cross_validation import train_test_split\n",
    "from sklearn import metrics, model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description of a pipeline. Used to apply sequence of transform. I had issue with cvec, needed to apply it two times on x_train and x_test, it gave bad results...**\n",
    "\n",
    "https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@chrisfotache/text-classification-in-python-pipelines-nlp-nltk-tf-idf-xgboost-and-more-b83451a327e0\n",
    "https://stackoverflow.com/questions/43366561/use-sklearns-gridsearchcv-with-a-pipeline-preprocessing-just-once\n",
    "\n",
    "https://www.kaggle.com/cesartrevisan/scikit-learn-and-gridsearchcv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Classifier Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best results we got so far were with tf-idf (even if the difference was not that significant with Countvectorizer) along with 100 000 features and use of unigram and bigram. Now we will compare using the same pipeline other models. Due to the expensive computation time, we will compare these models with a maximum of 10 000 features only and using search grid to get the best estimator each time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import multiprocessing\n",
    "import cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC #support vector machine SVM\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('clean_tweets')\n",
    "X = data.text.values\n",
    "y = data.sentiment.values\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Logistic Regression\", \"Linear SVC\", \"Random Forest\",\"Ridge Classifier\"]\n",
    "\n",
    "classifiers = [LogisticRegression(),LinearSVC(), RandomForestClassifier(), RidgeClassifier()]\n",
    "zipped_clf = zip(names,classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cythonmagic extension is already loaded. To reload it, use:\n",
      "  %reload_ext cythonmagic\n"
     ]
    }
   ],
   "source": [
    "%load_ext cythonmagic\n",
    "def classifier_comparator(vectorizer, ngram_range, classifier_list, n_features=10000, stop_words=None):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier_list:\n",
    "        checker_pipeline = Pipeline([\n",
    "            ('vectorizer', vectorizer),\n",
    "            ('classifier', c)\n",
    "        ])\n",
    "        \n",
    "        #----------- SEARCH GRID -----------\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Choosen classifier : {}\".format(n))\n",
    "        t0 = time()\n",
    "        if(n == \"Logistic Regression\") :\n",
    "            hyperparameters = dict(C=np.logspace(0, 4, 10), penalty=['l1', 'l2'])\n",
    "            clf = GridSearchCV(c, hyperparameters, cv=5, verbose=0)\n",
    "        elif(n == \"Random Forest\") :\n",
    "            hyperparameters = {\"max_depth\": [3, None]}\n",
    "        elif(n == \"Ridge Classifier\") :\n",
    "            hyperparameters = dict(alpha = np.array([0.01,0.001,0.0001]))\n",
    "        elif(n == \"Linear SVC\") :\n",
    "            Cs = [0.01, 0.1, 1]\n",
    "            hyperparameters = {'C': Cs}\n",
    "        \n",
    "        #search for best estimator\n",
    "        clf = GridSearchCV(c, hyperparameters, cv=5, verbose=0)\n",
    "        vectorizer.set_params(max_features=n_features, ngram_range=ngram_range)\n",
    "        best_estimator = Pipeline([('vectorizer', vectorizer),('classifier', clf)])\n",
    "        #best_estimator = Pipeline([('vectorizer', vectorizer),('classifier', c)])\n",
    "\n",
    "        #---------- predict with best model ------------\n",
    "        classifier_fit = best_estimator.fit(X_train, y_train)\n",
    "        y_pred = classifier_fit.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        tt_time = time() - t0\n",
    "        result.append([n, accuracy, tt_time])\n",
    "        print(\"Accuracy score is {} :\".format(accuracy))\n",
    "        print(\"Train and test time took {}\".format(tt_time))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Logistic Regression\n",
      "Accuracy score is 0.817795856107792 :\n",
      "Train and test time took 605.3928697109222\n",
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Linear SVC\n",
      "Accuracy score is 0.8174145163340536 :\n",
      "Train and test time took 40.70728349685669\n",
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Random Forest\n",
      "Accuracy score is 0.7755688318291598 :\n",
      "Train and test time took 321.2097415924072\n",
      "-----------------------------------------------------------------\n",
      "Choosen classifier : Ridge Classifier\n",
      "Accuracy score is 0.8126859031396975 :\n",
      "Train and test time took 461.7082438468933\n",
      "Wall time: 23min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#pool = multiprocessing.Pool(processes=2)\n",
    "#r = pool.map(classifier_comparator, years)\n",
    "#pool.close()\n",
    "all_results = classifier_comparator(tvec, range(1, 3), zipped_clf, n_features=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ada]",
   "language": "python",
   "name": "conda-env-ada-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
